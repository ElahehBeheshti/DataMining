{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOiC7xe2yPrwtUOydWUcNpr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElahehBeheshti/DataMining/blob/main/DMfinal_ChestXray.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNL4suRJYebf",
        "outputId": "ab54ec7f-13d6-4c10-884a-111de1820745"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Number of JPEG images in test/NORMAL: 234\n",
            "Number of JPEG images in test/PNEUMONIA: 390\n",
            "Number of JPEG images in train/NORMAL: 1341\n",
            "Number of JPEG images in train/PNEUMONIA: 3875\n",
            "Number of JPEG images in val/NORMAL: 8\n",
            "Number of JPEG images in val/PNEUMONIA: 8\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 2: Import necessary libraries\n",
        "import os\n",
        "\n",
        "# Step 3: Define the correct paths to the folders\n",
        "dataset_path = '/content/drive/My Drive/Chest-Xray'  # Update this with your correct path\n",
        "folders = {\n",
        "    'test': ['NORMAL', 'PNEUMONIA'],\n",
        "    'train': ['NORMAL', 'PNEUMONIA'],\n",
        "    'val': ['NORMAL', 'PNEUMONIA']\n",
        "}\n",
        "\n",
        "# Step 4: Function to count JPEG images in a folder\n",
        "def count_jpeg_images(folder_path):\n",
        "    count = 0\n",
        "    if os.path.exists(folder_path):  # Check if the folder path exists\n",
        "        for filename in os.listdir(folder_path):\n",
        "            if filename.lower().endswith('.jpeg') or filename.lower().endswith('.jpg'):\n",
        "                count += 1\n",
        "    else:\n",
        "        print(f\"Folder not found: {folder_path}\")\n",
        "    return count\n",
        "\n",
        "# Step 5: Loop through each folder and count images\n",
        "for main_folder, subfolders in folders.items():\n",
        "    for subfolder in subfolders:\n",
        "        folder_path = os.path.join(dataset_path, main_folder, subfolder)\n",
        "        num_images = count_jpeg_images(folder_path)\n",
        "        print(f\"Number of JPEG images in {main_folder}/{subfolder}: {num_images}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Preprocessing**\n",
        "\n",
        "Resize and Normalize Images: Use TensorFlow or OpenCV to resize images to 128x128 pixels and normalize pixel values.\n",
        "Data Augmentation: Set up data augmentation using ImageDataGenerator in Keras to generate more diverse training data."
      ],
      "metadata": {
        "id": "1p3y7PrYSEGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Data augmentation setup\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,  # Normalize pixel values\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Apply to training data\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, 'train'),\n",
        "    target_size=(128, 128),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agUlw_V1QgGs",
        "outputId": "1c613ba5-f06a-4a18-b71c-c03096e55a3e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5216 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Development**\n",
        "\n",
        "Load Pre-trained Models: Use DenseNet121 and EfficientNetB4 for transfer learning.\n",
        "\n",
        "We can customizing it for your binary classification task."
      ],
      "metadata": {
        "id": "Z2FI3O0aSi_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import DenseNet121\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Load pre-trained DenseNet121\n",
        "base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
        "base_model.trainable = False  # Freeze the base model\n",
        "\n",
        "# Add custom layers\n",
        "model = models.Sequential([\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(1, activation='sigmoid')  # Binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPDNyl_GSGwD",
        "outputId": "e16c9327-2ad9-46db-c481-1608ec8fe8ee"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m29084464/29084464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train the Model**\n",
        "\n",
        "***Set Class Weights: If there is class imbalance, calculate and use class weights.***\n",
        "\n",
        "Fit the Model: Use the model.fit() function to start training."
      ],
      "metadata": {
        "id": "g32VyWCCSsZj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate Class Weights:\n",
        "\n",
        "Suppose you have a list of class labels for all images in your training\n",
        "\n",
        "dataset (0 for \"Normal\" and 1 for \"Pneumonia\"). You can generate this list by\n",
        "\n",
        "examining your dataset."
      ],
      "metadata": {
        "id": "n4Reg0nWSxK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.utils import class_weight  # Import class_weight\n",
        "\n",
        "# Example: Assuming these are the total number of images in each class\n",
        "num_normal_images = 1341  # Total number of Normal images in the training set\n",
        "num_pneumonia_images = 3875  # Total number of Pneumonia images in the training set\n",
        "\n",
        "# Create a list of labels for the training set\n",
        "labels = np.array([0] * num_normal_images + [1] * num_pneumonia_images)\n",
        "\n",
        "# Calculate class weights\n",
        "class_weights = class_weight.compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(labels),\n",
        "    y=labels\n",
        ")\n",
        "\n",
        "# Convert to a dictionary\n",
        "class_weights = {0: class_weights[0], 1: class_weights[1]}\n",
        "\n",
        "print(class_weights)  # Output the class weights\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VwpuDarSlyw",
        "outputId": "02220ba6-d8a6-4864-b199-5d764ea1d54f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 1.9448173005219984, 1: 0.6730322580645162}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a Validation Data Generator\n"
      ],
      "metadata": {
        "id": "DM41l6ylTEXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Create an instance of ImageDataGenerator for validation data (only rescaling, no augmentation)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Define the validation data generator\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, 'val'),  # Path to the validation data\n",
        "    target_size=(128, 128),  # Resize images to 128x128\n",
        "    batch_size=32,  # You can adjust the batch size as needed\n",
        "    class_mode='binary'  # Use 'binary' for binary classification\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbHhZnymS0DS",
        "outputId": "6bb8a2c5-eb96-42d4-9c91-a9086e5c5c96"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 16 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Updated Model Training Code\n"
      ],
      "metadata": {
        "id": "MQbEvH5ITWnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model with class weights and the validation generator\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=10,\n",
        "    class_weight=class_weights,  # Apply class weights\n",
        "    validation_data=val_generator  # Use the validation data generator\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkezKMeCTXOP",
        "outputId": "ffac429e-4762-4483-fb41-804c93c83f48"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m517s\u001b[0m 3s/step - accuracy: 0.7988 - loss: 0.4942 - val_accuracy: 0.9375 - val_loss: 0.2602\n",
            "Epoch 2/10\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m390s\u001b[0m 2s/step - accuracy: 0.9016 - loss: 0.2433 - val_accuracy: 0.8750 - val_loss: 0.3104\n",
            "Epoch 3/10\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m436s\u001b[0m 2s/step - accuracy: 0.9096 - loss: 0.2305 - val_accuracy: 0.7500 - val_loss: 0.4172\n",
            "Epoch 4/10\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m384s\u001b[0m 2s/step - accuracy: 0.9022 - loss: 0.2320 - val_accuracy: 0.8125 - val_loss: 0.3303\n",
            "Epoch 5/10\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m439s\u001b[0m 2s/step - accuracy: 0.9152 - loss: 0.2122 - val_accuracy: 0.8125 - val_loss: 0.3604\n",
            "Epoch 6/10\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m374s\u001b[0m 2s/step - accuracy: 0.9183 - loss: 0.1974 - val_accuracy: 0.6250 - val_loss: 0.4790\n",
            "Epoch 7/10\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 2s/step - accuracy: 0.9235 - loss: 0.1957 - val_accuracy: 0.8750 - val_loss: 0.2578\n",
            "Epoch 8/10\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m379s\u001b[0m 2s/step - accuracy: 0.9142 - loss: 0.2019 - val_accuracy: 0.9375 - val_loss: 0.2636\n",
            "Epoch 9/10\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m378s\u001b[0m 2s/step - accuracy: 0.9248 - loss: 0.1860 - val_accuracy: 0.6250 - val_loss: 0.5150\n",
            "Epoch 10/10\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m376s\u001b[0m 2s/step - accuracy: 0.9209 - loss: 0.2025 - val_accuracy: 0.6875 - val_loss: 0.4400\n"
          ]
        }
      ]
    }
  ]
}